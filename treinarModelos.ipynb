{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8cd5c9-fee6-4564-8736-438c67979782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 15:20:18.905485: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-07 15:20:18.996919: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-07 15:20:19.021903: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-07 15:20:19.178389: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-07 15:20:20.453119: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import h5py\n",
    "import csv\n",
    "import time\n",
    "import gc\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Input, Concatenate, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1128523a-a737-4d05-8a04-4d461db04a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_pointer(fp, tam):\n",
    "    lines = [ll.strip() for ll in fp]\n",
    "    ii = 0\n",
    "    labels = []\n",
    "    res = []\n",
    "    cli = []\n",
    "    numLinhas = 0\n",
    "    while ii < len(lines):\n",
    "        line = lines[ii]\n",
    "        #contando o numero de vertices do grafo\n",
    "        if \"cliqueatual\" not in line:\n",
    "            ii += 1\n",
    "            numLinhas += 1\n",
    "            continue\n",
    "\n",
    "        #pegando a clique atual\n",
    "        if ii+1 >= len(lines):\n",
    "            break\n",
    "        line = line[3:]\n",
    "        spritado = line.split()\n",
    "        clique = [int(elem) for elem in spritado[1:]]\n",
    "        if(numLinhas < tam):\n",
    "            dif = tam - numLinhas\n",
    "            clique.extend([0]*dif)\n",
    "        cli.append(clique)\n",
    "\n",
    "        #criando o vetor de movimento\n",
    "        line = lines[ii+1]\n",
    "        sp = line.split()\n",
    "        mv = int(sp[-1])\n",
    "        label = [0] * tam\n",
    "        label[mv] = 1\n",
    "        labels.append(label)\n",
    "\n",
    "        #lendo o grafo\n",
    "        cells = []\n",
    "        for tt in range(numLinhas, 0, -1):\n",
    "            cell_line = lines[ii - tt][3:]\n",
    "            cells.extend([int(float(cc)) for cc in cell_line.split(\", \")])\n",
    "            if(numLinhas < tam):\n",
    "                dif = tam - numLinhas\n",
    "                cells.extend([0]*dif)\n",
    "        while len(cells) < tam * tam:\n",
    "            cells.extend([0]*tam)\n",
    "\n",
    "        #cells = np.reshape(cells,((tam,  -1)))\n",
    "        #cells = np.transpose(cells)\n",
    "        #cells = np.reshape(cells, -1)\n",
    "        res.append(cells)\n",
    "        ii += (numLinhas+2)\n",
    "    labels_v = list(range(len(labels),0, -1))\n",
    "    return (res, cli, labels, labels_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec6a7c5-ea3e-47a2-a1d3-d168e03916d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dir(ddir, tam):\n",
    "    res = []\n",
    "    cli = []\n",
    "    labels = []\n",
    "    labels_v = []\n",
    "    random.seed(42)\n",
    "    files = sorted([os.path.basename(ii) for ii in glob.glob(\"{0}/*.dimacs\".format(ddir))])\n",
    "    random.shuffle(files)\n",
    "    random.seed()\n",
    "    i = 0\n",
    "    for ff in files:\n",
    "        with open(os.path.join(ddir,ff), 'r') as fp:\n",
    "            rr, cc, ll, ll_v = parse_file_pointer(fp, tam)\n",
    "            res.extend(rr)\n",
    "            cli.extend(cc)\n",
    "            labels.extend(ll)\n",
    "            labels_v.extend(ll_v)\n",
    "        i+=1\n",
    "        if i > 5:\n",
    "            break\n",
    "    return res, cli, labels, labels_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae619989-ab64-4168-9a12-bc575a916901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class printbatch(callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        logging.info(\"Epoch: \"+ str(epoch))\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logging.info(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81e2016e-cf70-4e08-9a80-9d0159c2ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggerWriter:\n",
    "    def __init__(self, level):\n",
    "        self.level = level\n",
    "\n",
    "    def write(self, message):\n",
    "        if message != '\\n':\n",
    "            self.level(message)\n",
    "\n",
    "    def flush(self):\n",
    "        self.level(sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2809c21c-f5cb-4135-9ae5-e19e265a00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(data, clique, labels, tam, output_path, shared_layer_multipliers, layer_multipliers, batch_size, learning_rate):\n",
    "    \n",
    "    shared_layer_multipliers = [x for x in shared_layer_multipliers if x != 0]\n",
    "\n",
    "    # Definir camadas de entrada\n",
    "    graph_input = [Input(shape=(tam,)) for _ in range(tam)]  # Matriz de adjacência\n",
    "    clique_input = Input(shape=(tam,))  # Vetor de clique\n",
    "\n",
    "    # Camadas densas compartilhadas\n",
    "    graph_input_processed = graph_input\n",
    "    clique_input_processed = clique_input\n",
    "    for i in range(len(shared_layer_multipliers)):\n",
    "        shared_dense_graph = Dense(tam * shared_layer_multipliers[i], activation='relu')\n",
    "        shared_dense_clique = Dense(tam * shared_layer_multipliers[i], activation='relu')\n",
    "        graph_input_processed = [shared_dense_graph(inp) for inp in graph_input_processed]\n",
    "        clique_input_processed = shared_dense_clique(clique_input_processed)\n",
    "    \n",
    "    # Concatenar os vetores processados\n",
    "    merged_vector = Concatenate(axis=-1)(graph_input_processed + [clique_input_processed])\n",
    "    \n",
    "    #camadas internas\n",
    "    layer = merged_vector\n",
    "    for i in range(len(layer_multipliers)):\n",
    "        layer = Dense(tam*layer_multipliers[i],activation='relu')(layer)\n",
    "\n",
    "    #camada de saida\n",
    "    output_layer = Dense(tam, activation='softmax')(layer)\n",
    "    \n",
    "    #compilar modelo\n",
    "    model = Model(inputs=[graph_input, clique_input], outputs=output_layer)\n",
    "    adam = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    #treinar modelo\n",
    "    now = datetime.now()\n",
    "    model.fit([np.hsplit(data, tam), clique], labels, epochs=1000, batch_size=batch_size,validation_split=0.2,verbose=2,\n",
    "              callbacks=[printbatch(), EarlyStopping(monitor='val_loss', patience=50, verbose=0), ModelCheckpoint(os.path.join(output_path, \"models\",\n",
    "                            \"dnn_model_\" + str(tam) + \"_\"+ str(now.day) + \".\" + str(now.month) + \".\" + str(now.year) + \"_\"\n",
    "                            + \"_{epoch:02d}-{val_loss:.2f}\" + \".h5\"),\n",
    "                            monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')])\n",
    "    model.save(os.path.join(output_path, \"dnn_model_\" + str(tam)+\"_\"+ str(now.day) + \".\" + str(now.month) + \".\" + str(now.year) +\"_\"+ \".h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b339dcc-d277-407b-8efc-e54f05d09306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_value(data, clique, labels, tam, output_path, shared_layer_multipliers, layer_multipliers, batch_size, learning_rate):\n",
    "   \n",
    "    shared_layer_multipliers = [x for x in shared_layer_multipliers if x != 0]\n",
    "\n",
    "    # Definir camadas de entrada\n",
    "    graph_input = [Input(shape=(tam,)) for _ in range(tam)]  # Matriz de adjacência\n",
    "    clique_input = Input(shape=(tam,))  # Vetor de clique\n",
    "\n",
    "    # Camadas densas compartilhadas\n",
    "    graph_input_processed = graph_input\n",
    "    clique_input_processed = clique_input\n",
    "    for i in range(len(shared_layer_multipliers)):\n",
    "        shared_dense_graph = Dense(tam * shared_layer_multipliers[i], activation='relu')\n",
    "        shared_dense_clique = Dense(tam * shared_layer_multipliers[i], activation='relu')\n",
    "        graph_input_processed = [shared_dense_graph(inp) for inp in graph_input_processed]\n",
    "        clique_input_processed = shared_dense_clique(clique_input_processed)\n",
    "    \n",
    "    # Concatenar os vetores processados\n",
    "    merged_vector = Concatenate(axis=-1)(graph_input_processed + [clique_input_processed])\n",
    "    \n",
    "    #camadas internas\n",
    "    layer = merged_vector\n",
    "    for i in range(len(layer_multipliers)):\n",
    "        layer = Dense(tam*layer_multipliers[i],activation='relu')(layer)\n",
    "\n",
    "    #camada de saida\n",
    "    output_layer = Dense(1)(layer)\n",
    "\n",
    "    #compilar modelo\n",
    "    model = Model(inputs=[graph_input, clique_input], outputs=output_layer)\n",
    "    adam = optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam, loss='mse', metrics=['mae'])\n",
    "\n",
    "    #treinar modelo\n",
    "    now = datetime.datetime.now()\n",
    "    model.fit([np.hsplit(data, tam), clique], labels, epochs=1000, batch_size=batch_size, validation_split=0.2, verbose=2,\n",
    "              callbacks=[printbatch(), EarlyStopping(monitor='val_loss', patience=50, verbose=0), ModelCheckpoint(os.path.join(output_path, \"models\",\n",
    "                            \"dnn_value_model_\" + str(tam) + \"x\" +\"_\"+ str(now.day) + \".\" + str(now.month) + \".\" + str(now.year) + \"_\" +\n",
    "                            \"_{epoch:02d}-{val_loss:.2f}\" + \".h5\"), monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False,\n",
    "                            mode='auto')])\n",
    "    model.save(os.path.join(output_path, \"dnn_value_model_\"+str(tam)+\"_\"+str(now.day)+\".\"+str(now.month)+\".\"+str(now.year)+\"_\"+\".h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f3a334-c5de-4170-b29e-9ca2a5a25ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leu tudo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731003753.182851    6063 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1731003753.354421    6063 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1731003753.354471    6063 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1731003753.358751    6063 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1731003753.358883    6063 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1731003753.358912    6063 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1731003753.570845    6063 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1731003753.570911    6063 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-07 15:22:33.570925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1731003753.570970    6063 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-07 15:22:33.571589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-11-07 15:22:44.868330: W external/local_tsl/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.58GiB (rounded to 1694250240)requested by op AddV2\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-11-07 15:22:44.868384: I external/local_tsl/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2024-11-07 15:22:44.868395: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 11, Chunks in use: 11. 2.8KiB allocated for chunks. 2.8KiB in use in bin. 68B client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868400: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868405: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868410: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 2, Chunks in use: 2. 6.0KiB allocated for chunks. 6.0KiB in use in bin. 5.9KiB client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868415: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 4, Chunks in use: 4. 20.0KiB allocated for chunks. 20.0KiB in use in bin. 19.5KiB client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868419: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868423: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868427: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868431: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868435: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868439: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868443: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868447: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 3, Chunks in use: 2. 4.27MiB allocated for chunks. 2.86MiB in use in bin. 2.86MiB client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868452: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 5, Chunks in use: 2. 14.30MiB allocated for chunks. 5.72MiB in use in bin. 5.72MiB client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868457: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 2, Chunks in use: 2. 11.44MiB allocated for chunks. 11.44MiB in use in bin. 11.44MiB client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868461: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868465: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868471: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868475: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868478: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868483: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 2. 3.93GiB allocated for chunks. 3.16GiB in use in bin. 3.16GiB client-requested in use in bin.\n",
      "2024-11-07 15:22:44.868488: I external/local_tsl/tsl/framework/bfc_allocator.cc:1062] Bin for 1.58GiB was 256.00MiB, Chunk State: \n",
      "2024-11-07 15:22:44.868495: I external/local_tsl/tsl/framework/bfc_allocator.cc:1068]   Size: 795.43MiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 1.58GiB | Requested Size: 1.58GiB | in_use: 1 | bin_num: -1\n",
      "2024-11-07 15:22:44.868499: I external/local_tsl/tsl/framework/bfc_allocator.cc:1075] Next region of size 4254072832\n",
      "2024-11-07 15:22:44.868504: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 704c00000 of size 256 next 1\n",
      "2024-11-07 15:22:44.868508: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 704c00100 of size 1280 next 2\n",
      "2024-11-07 15:22:44.868511: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 704c00600 of size 256 next 3\n",
      "2024-11-07 15:22:44.868514: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 704c00700 of size 256 next 4\n",
      "2024-11-07 15:22:44.868517: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 704c00800 of size 256 next 5\n",
      "2024-11-07 15:22:44.868520: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 704c00900 of size 6144 next 7\n",
      "2024-11-07 15:22:44.868524: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 704c02100 of size 2994944 next 10\n",
      "2024-11-07 15:22:44.868527: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 704edd400 of size 1500160 next 11\n",
      "2024-11-07 15:22:44.868531: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70504b800 of size 6144 next 6\n",
      "2024-11-07 15:22:44.868534: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70504d000 of size 256 next 9\n",
      "2024-11-07 15:22:44.868537: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70504d100 of size 256 next 8\n",
      "2024-11-07 15:22:44.868540: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70504d200 of size 4096 next 12\n",
      "2024-11-07 15:22:44.868543: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70504e200 of size 4096 next 15\n",
      "2024-11-07 15:22:44.868546: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70504f200 of size 3072 next 20\n",
      "2024-11-07 15:22:44.868549: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 70504fe00 of size 3072 next 22\n",
      "2024-11-07 15:22:44.868553: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 705050a00 of size 256 next 16\n",
      "2024-11-07 15:22:44.868556: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 705050b00 of size 256 next 17\n",
      "2024-11-07 15:22:44.868559: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 705050c00 of size 256 next 25\n",
      "2024-11-07 15:22:44.868562: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 705050d00 of size 256 next 27\n",
      "2024-11-07 15:22:44.868565: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 705050e00 of size 256 next 28\n",
      "2024-11-07 15:22:44.868568: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 705050f00 of size 1478144 next 13\n",
      "2024-11-07 15:22:44.868572: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7051b9d00 of size 1500160 next 14\n",
      "2024-11-07 15:22:44.868575: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 705328100 of size 3000064 next 26\n",
      "2024-11-07 15:22:44.868578: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 705604800 of size 3000064 next 24\n",
      "2024-11-07 15:22:44.868582: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 7058e0f00 of size 3000064 next 23\n",
      "2024-11-07 15:22:44.868585: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 705bbd600 of size 3000064 next 19\n",
      "2024-11-07 15:22:44.868588: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 705e99d00 of size 6000128 next 18\n",
      "2024-11-07 15:22:44.868591: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 706452b00 of size 6000128 next 21\n",
      "2024-11-07 15:22:44.868595: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 706a0b900 of size 1694250240 next 29\n",
      "2024-11-07 15:22:44.868598: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] InUse at 76b9cee00 of size 1694250240 next 31\n",
      "2024-11-07 15:22:44.868601: I external/local_tsl/tsl/framework/bfc_allocator.cc:1095] Free  at 7d0992300 of size 834067712 next 18446744073709551615\n",
      "2024-11-07 15:22:44.868605: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2024-11-07 15:22:44.868609: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 11 Chunks of size 256 totalling 2.8KiB\n",
      "2024-11-07 15:22:44.868613: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-11-07 15:22:44.868616: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 3072 totalling 6.0KiB\n",
      "2024-11-07 15:22:44.868620: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 4096 totalling 8.0KiB\n",
      "2024-11-07 15:22:44.868624: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 6144 totalling 12.0KiB\n",
      "2024-11-07 15:22:44.868627: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 1500160 totalling 2.86MiB\n",
      "2024-11-07 15:22:44.868631: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 3000064 totalling 5.72MiB\n",
      "2024-11-07 15:22:44.868635: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 6000128 totalling 11.44MiB\n",
      "2024-11-07 15:22:44.868638: I external/local_tsl/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 1694250240 totalling 3.16GiB\n",
      "2024-11-07 15:22:44.868642: I external/local_tsl/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 3.17GiB\n",
      "2024-11-07 15:22:44.868646: I external/local_tsl/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 4254072832 memory_limit_: 4254072832 available bytes: 0 curr_region_allocation_bytes_: 8508145664\n",
      "2024-11-07 15:22:44.868653: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                      4254072832\n",
      "InUse:                      3409531904\n",
      "MaxInUse:                   3409532160\n",
      "NumAllocs:                          77\n",
      "MaxAllocSize:               1694250240\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-11-07 15:22:44.868658: W external/local_tsl/tsl/framework/bfc_allocator.cc:494] *********************************************************************************___________________\n",
      "2024-11-07 15:22:44.868669: W tensorflow/core/framework/op_kernel.cc:1828] RESOURCE_EXHAUSTED: failed to allocate memory\n",
      "2024-11-07 15:22:44.868683: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m         learn_value(np\u001b[38;5;241m.\u001b[39marray(res), np\u001b[38;5;241m.\u001b[39marray(clique), np\u001b[38;5;241m.\u001b[39marray(labels_v), tam, output_path, param_v_a_1, param_v_a_2, param_v_b, param_v_l)\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrede 2 treinada\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m res, clique, labels, labels_v \u001b[38;5;241m=\u001b[39m parse_dir(labeled_data_dir, tam)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleu tudo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclique\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_p_a_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_p_a_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_p_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_p_l\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrede 1 treinada\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#treinar rede bound\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m, in \u001b[0;36mlearn\u001b[0;34m(data, clique, labels, tam, output_path, shared_layer_multipliers, layer_multipliers, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     22\u001b[0m layer \u001b[38;5;241m=\u001b[39m merged_vector\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(layer_multipliers)):\n\u001b[0;32m---> 24\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtam\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlayer_multipliers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#camada de saida\u001b[39;00m\n\u001b[1;32m     27\u001b[0m output_layer \u001b[38;5;241m=\u001b[39m Dense(tam, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)(layer)\n",
      "File \u001b[0;32m~/anaconda3/envs/tcc/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tcc/lib/python3.9/site-packages/keras/src/backend/tensorflow/random.py:34\u001b[0m, in \u001b[0;36muniform\u001b[0;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[1;32m     32\u001b[0m dtype \u001b[38;5;241m=\u001b[39m dtype \u001b[38;5;129;01mor\u001b[39;00m floatx()\n\u001b[1;32m     33\u001b[0m seed \u001b[38;5;241m=\u001b[39m _cast_seed(draw_seed(seed))\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    output_path = \"modelos\" #caminha onde é salvo o modelo\n",
    "    labeled_data_dir = \"train_graphs\" #caminho dos dados para treinar o modelo\n",
    "    param_v_a_1 = [4, 3, 2] #camadas compartilhadas rede bound\n",
    "    param_v_a_2 = [3, 2, 2] #camadas ocultas\n",
    "    param_p_a_1 = [6, 4, 3] #camadas compartilhadas rede brach\n",
    "    param_p_a_2 = [9, 6, 2] #camadas ocultas\n",
    "    param_p_b = 512 #batch size\n",
    "    param_v_b =  512\n",
    "    param_p_l = 0.001 #taxa de aprendizado\n",
    "    param_v_l = 0.001 \n",
    "    tam = 250\n",
    "    use_value_model = True #se vai treinar a rede de bound\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    #treinar rede branch\n",
    "    res, clique, labels, labels_v = parse_dir(labeled_data_dir, tam)\n",
    "    print(\"leu tudo\")\n",
    "    learn(np.array(res), np.array(clique), np.array(labels), tam, output_path, param_p_a_1, param_p_a_2, param_p_b, param_p_l)\n",
    "    print(\"rede 1 treinada\")\n",
    "    #treinar rede bound\n",
    "    if use_value_model:\n",
    "        learn_value(np.array(res), np.array(clique), np.array(labels_v), tam, output_path, param_v_a_1, param_v_a_2, param_v_b, param_v_l)\n",
    "        print(\"rede 2 treinada\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
