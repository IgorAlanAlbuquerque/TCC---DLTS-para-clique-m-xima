{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8cd5c9-fee6-4564-8736-438c67979782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-25 20:01:09.465173: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-25 20:01:09.579924: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-25 20:01:09.609526: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-25 20:01:09.815319: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-25 20:01:11.146773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, top_k_accuracy_score, r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e151c0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1735167674.658867    4301 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1735167674.886538    4301 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1735167674.886653    4301 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "# Limpa a sessão antes de construir o modelo\n",
    "clear_session()\n",
    "\n",
    "# Configurações de GPU\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "# Define a política de precisão mista\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254d8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"modelos\" #caminha onde é salvo o modelo\n",
    "files_per_batch = 1000\n",
    "labeled_data_dir = \"train_graphs\" #caminho dos dados para treinar o modelo\n",
    "param_v_a_1 = [4, 3, 2] #camadas compartilhadas rede bound\n",
    "param_v_a_2 = [3, 2, 2] #camadas ocultas\n",
    "param_p_a_1 = [6, 4, 3] #camadas compartilhadas rede brach\n",
    "param_p_a_2 = [9, 6, 2] #camadas ocultas\n",
    "param_p_b = 250 #batch size\n",
    "param_v_b =  250\n",
    "param_p_l = 0.001 #taxa de aprendizado\n",
    "param_v_l = 0.001\n",
    "tam = 120\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac49ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando todos os arquivos do diretório\n",
    "files = sorted([os.path.basename(ii) for ii in glob.glob(f\"{labeled_data_dir}/*.dimacs\")])\n",
    "files_treino, files_teste = train_test_split(files, test_size=0.1, random_state=42)\n",
    "files_t, files_v = train_test_split(files_treino, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1128523a-a737-4d05-8a04-4d461db04a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_pointer(fp):\n",
    "    lines = [ll.strip() for ll in fp]\n",
    "    ii = 0\n",
    "    labels = []\n",
    "    res = []\n",
    "    cli = []\n",
    "    numLinhas = 0\n",
    "    while ii < len(lines):\n",
    "        line = lines[ii]\n",
    "        #contando o numero de vertices do grafo\n",
    "        if \"cliqueatual\" not in line:\n",
    "            ii += 1\n",
    "            numLinhas += 1\n",
    "            continue\n",
    "\n",
    "        #pegando a clique atual\n",
    "        if ii+1 >= len(lines):\n",
    "            break\n",
    "        line = line[3:]\n",
    "        spritado = line.split()\n",
    "        clique = [int(elem) for elem in spritado[1:]]\n",
    "        if(numLinhas < tam):\n",
    "            dif = tam - numLinhas\n",
    "            clique.extend([0]*dif)\n",
    "        cli.append(clique)\n",
    "\n",
    "        #criando o vetor de movimento\n",
    "        line = lines[ii+1]\n",
    "        sp = line.split()\n",
    "        mv = int(sp[-1])\n",
    "        label = [0] * tam\n",
    "        label[mv] = 1\n",
    "        labels.append(label)\n",
    "\n",
    "        #lendo o grafo\n",
    "        cells = []\n",
    "        for tt in range(numLinhas, 0, -1):\n",
    "            cell_line = lines[ii - tt][3:]\n",
    "            cells.extend([int(float(cc)) for cc in cell_line.split(\", \")])\n",
    "            if(numLinhas < tam):\n",
    "                dif = tam - numLinhas\n",
    "                cells.extend([0]*dif)\n",
    "        while len(cells) < tam * tam:\n",
    "            cells.extend([0]*tam)\n",
    "        res.append(cells)\n",
    "        ii += (numLinhas+2)\n",
    "    labels_v = list(range(len(labels),0, -1))\n",
    "    return (res, cli, labels, labels_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82174a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estruturar_entrada(batch_input, batch_labels):\n",
    "    # Dividir a entrada em uma lista de 151 tensores de forma (batch_size, tam)\n",
    "    batch_input_list = [batch_input[:, i, :] for i in range(batch_input.shape[1])]\n",
    "            \n",
    "    # Converter para tensores do TensorFlow\n",
    "    x_batch = [tf.convert_to_tensor(tensor, dtype=tf.float32) for tensor in batch_input_list]\n",
    "    input_dict = {f'input_{i}': tensor for i, tensor in enumerate(x_batch)}\n",
    "    y_batch = np.array(batch_labels)\n",
    "    return input_dict, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa2ee89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinar_entrada(res, clique, labels, remaining_batch_input=[], remaining_batch_labels=[]):\n",
    "    combined_input = np.array([np.hsplit(np.concatenate([res[i], clique[i]]), tam + 1) for i in range(len(clique))])\n",
    "    if len(remaining_batch_input) != 0:\n",
    "        combined_input = np.concatenate((remaining_batch_input, combined_input), axis=0)\n",
    "        labels = remaining_batch_labels + labels\n",
    "    return combined_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec6a7c5-ea3e-47a2-a1d3-d168e03916d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dir(files):\n",
    "    res = []\n",
    "    cli = []\n",
    "    labels = []\n",
    "    labels_v = []\n",
    "    random.seed(42)\n",
    "    random.shuffle(files)\n",
    "    random.seed()\n",
    "    for ff in files:\n",
    "        with open(os.path.join(labeled_data_dir,ff), 'r') as fp:\n",
    "            rr, cc, ll, ll_v = parse_file_pointer(fp)\n",
    "            res.extend(rr)\n",
    "            cli.extend(cc)\n",
    "            labels.extend(ll)\n",
    "            labels_v.extend(ll_v)\n",
    "    return res, cli, labels, labels_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa8fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_arquivos(batch_files, value):\n",
    "    res, clique, labels, labels_v = parse_dir(batch_files)\n",
    "    if value:\n",
    "        labels = labels_v\n",
    "    return res, clique, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98d3c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, value):\n",
    "    # Gerador de treinamento\n",
    "    def create_generator(file_list):\n",
    "        def generator():\n",
    "            idx = 0\n",
    "            remaining_batch_input = remaining_batch_labels = []\n",
    "            while idx < len(file_list):\n",
    "                batch_files = file_list[idx: idx + files_per_batch]\n",
    "                res, clique, labels = ler_arquivos(batch_files, value)\n",
    "                combined_input, labels = combinar_entrada(res, clique, labels, remaining_batch_input, remaining_batch_labels)\n",
    "                remaining_batch_input = remaining_batch_labels = []\n",
    "                num_samples = len(combined_input)\n",
    "                num_batches = num_samples // batch_size\n",
    "                for batch_idx in range(num_batches):\n",
    "                    batch_input = combined_input[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
    "                    batch_labels = labels[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
    "                    x_batch, y_batch = estruturar_entrada(batch_input, batch_labels)\n",
    "                    yield x_batch, y_batch\n",
    "                idx += files_per_batch\n",
    "                remaining_samples = num_samples % batch_size\n",
    "                if remaining_samples > 0:\n",
    "                    remaining_batch_input = combined_input[-remaining_samples:]\n",
    "                    remaining_batch_labels = labels[-remaining_samples:]\n",
    "        return generator\n",
    "    \n",
    "    #ler o dataset de validação todo de uma vez ao invés de usar um gerador pra ele\n",
    "    res, clique, labels = ler_arquivos(files_v, value)\n",
    "    combined_input, labels = combinar_entrada(res, clique, labels)\n",
    "    x_value, y_value = estruturar_entrada(combined_input, labels)\n",
    "    \n",
    "    return create_generator(files_t)(), x_value, y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "739d1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_signature_type_branch(batch_size):\n",
    "    output_signature=(\n",
    "            {f\"input_{i}\": tf.TensorSpec(shape=(batch_size, tam), dtype=tf.float32) for i in range(tam + 1)},  # Entradas\n",
    "            tf.TensorSpec(shape=(batch_size, tam), dtype=tf.float32)  # Rótulos\n",
    "    )\n",
    "    return output_signature\n",
    "\n",
    "def output_signature_type_bound(batch_size):\n",
    "    output_signature=(\n",
    "            {f\"input_{i}\": tf.TensorSpec(shape=(batch_size, tam), dtype=tf.float32) for i in range(tam + 1)},  # Entradas\n",
    "            tf.TensorSpec(shape=(batch_size,), dtype=tf.float32)  # Rótulos\n",
    "    )\n",
    "    return output_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae619989-ab64-4168-9a12-bc575a916901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class printbatch(callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        logging.info(\"Epoch: \"+ str(epoch))\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logging.info(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2809c21c-f5cb-4135-9ae5-e19e265a00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#treinar rede branch\n",
    "#verificar se já tem um modelo compilado para treinar\n",
    "initial_epoch = 0\n",
    "checkpoint_dir = os.path.join(output_path, \"checkpoints\", f\"dnn_model_{tam}\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint_epoch_{epoch:04d}.keras\")\n",
    "\n",
    "if latest_checkpoint:\n",
    "    print(f\"Carregando modelo do checkpoint: {latest_checkpoint}\")\n",
    "    model = tf.keras.models.load_model(latest_checkpoint)\n",
    "    # Extraindo a última época treinada\n",
    "    initial_epoch = int(latest_checkpoint.split(\"_epoch_\")[1].split(\".keras\")[0])\n",
    "\n",
    "elif os.path.exists(os.path.join(checkpoint_dir, 'compiled_model.keras')):\n",
    "    print(\"Carregando modelo compilado salvo...\")\n",
    "    model = tf.keras.models.load_model(os.path.join(checkpoint_dir, 'compiled_model.keras'))\n",
    "\n",
    "else:\n",
    "    # Definir camadas de entrada\n",
    "    inputArray = [Input(shape=(tam,), name=f\"input_{i}\") for i in range(tam + 1)]\n",
    "\n",
    "    # Camadas densas compartilhadas\n",
    "    layer = inputArray\n",
    "    for i in range(len(param_p_a_1)):\n",
    "        shared_dense = Dense(tam * param_p_a_1[i], activation='relu')\n",
    "        layer = [shared_dense(l) for l in layer]\n",
    "\n",
    "    # Concatenar os vetores processados\n",
    "    merged_vector = Concatenate(axis=-1)(layer)\n",
    "\n",
    "    #camadas internas\n",
    "    layer = merged_vector\n",
    "    for i in range(len(param_p_a_2)):\n",
    "        layer = Dense(tam*(tam+1)*param_p_a_2[i],activation='relu')(layer)\n",
    "\n",
    "    #camada de saida\n",
    "    output_layer = Dense(tam, activation='softmax')(layer)\n",
    "\n",
    "    #compilar modelo\n",
    "    model = Model(inputs=inputArray, outputs=output_layer)\n",
    "    adam = optimizers.Adam(learning_rate=param_p_l)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "# Criar os datasets de treino e validação\n",
    "train_generator_func, val_x, val_y = data_generator(param_p_b, False)\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda : train_generator_func, output_signature=output_signature_type_branch(param_p_b))\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "now = datetime.now()\n",
    "# Treinar modelo\n",
    "print(\"treinamento rede branch iniciado\")\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1000,\n",
    "    initial_epoch=initial_epoch,\n",
    "    verbose=1,\n",
    "    validation_data=(val_x, val_y),\n",
    "    callbacks=[\n",
    "        printbatch(),\n",
    "        EarlyStopping(monitor='val_loss', patience=50, verbose=0),\n",
    "        ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=False, save_weights_only=False)\n",
    "    ]\n",
    ")\n",
    "final_model_dnn_path = os.path.join(output_path, f\"dnn_model_{tam}_{now.day}.{now.month}.{now.year}.keras\")\n",
    "model.save(final_model_dnn_path)\n",
    "del model, val_x, val_y, train_dataset, train_generator_func, inputArray, merged_vector, layer, output_layer\n",
    "print(\"rede 1 treinada\")\n",
    "clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b339dcc-d277-407b-8efc-e54f05d09306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#treinar rede bound\n",
    "checkpoint_dir = os.path.join(output_path, \"checkpoints\", f\"dnn_value_model_{tam}\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint_epoch_{epoch:04d}.keras\")\n",
    "\n",
    "# Verificar se existe um checkpoint salvo\n",
    "initial_epoch = 0\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if latest_checkpoint:\n",
    "    print(f\"Carregando modelo do checkpoint: {latest_checkpoint}\")\n",
    "    model = tf.keras.models.load_model(latest_checkpoint)\n",
    "    # Extraindo a última época treinada\n",
    "    initial_epoch = int(latest_checkpoint.split(\"_epoch_\")[1].split(\".keras\")[0])\n",
    "\n",
    "elif os.path.exists(os.path.join(checkpoint_dir, 'compiled_model.keras')):\n",
    "    print(\"Carregando modelo compilado salvo...\")\n",
    "    model = tf.keras.models.load_model(os.path.join(checkpoint_dir, 'compiled_model.keras'))\n",
    "\n",
    "else: \n",
    "    # Definir camadas de entrada\n",
    "    inputArray = [Input(shape=(tam,), name=f\"input_{i}\") for i in range(tam + 1)]\n",
    "\n",
    "    # Camadas densas compartilhadas\n",
    "    layer = inputArray\n",
    "    for i in range(len(param_v_a_1)):\n",
    "        shared_dense = Dense(tam * param_v_a_1[i], activation='relu')\n",
    "        layer = [shared_dense(l) for l in layer]\n",
    "\n",
    "    # Concatenar os vetores processados\n",
    "    merged_vector = Concatenate(axis=-1)(layer)\n",
    "\n",
    "    #camadas internas\n",
    "    layer = merged_vector\n",
    "    for i in range(len(param_v_a_2)):\n",
    "        layer = Dense(tam*(tam+1)*param_v_a_2[i],activation='relu')(layer)\n",
    "\n",
    "    #camada de saida\n",
    "    output_layer = Dense(1)(layer)\n",
    "\n",
    "    #compilar modelo\n",
    "    model = Model(inputs=inputArray, outputs=output_layer)\n",
    "    adam = optimizers.Adam(learning_rate=param_v_l)\n",
    "    model.compile(optimizer=adam, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Criar os datasets de treino e validação\n",
    "train_generator_func, val_x, val_y = data_generator(param_v_b, True)\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda : train_generator_func, output_signature=output_signature_type_bound(param_v_b))\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "now = datetime.now()\n",
    "# Treinar modelo\n",
    "print(\"treinamento rede bound iniciado\")\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1000,\n",
    "    initial_epoch=initial_epoch,\n",
    "    verbose=1,\n",
    "    validation_data=(val_x, val_y),\n",
    "    callbacks=[\n",
    "        printbatch(),\n",
    "        EarlyStopping(monitor='val_loss', patience=50, verbose=0),\n",
    "        ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=False)\n",
    "    ]\n",
    ")\n",
    "final_model_value_path = os.path.join(output_path, f\"dnn_value_model_{tam}_{now.day}.{now.month}.{now.year}.keras\")\n",
    "model.save(final_model_value_path)\n",
    "del model, val_x, val_y, train_dataset, train_generator_func, inputArray, merged_vector, layer, output_layer\n",
    "print(\"rede 2 treinada\")\n",
    "clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ac871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia\n",
    "'''def calcular_acuracia(true_values, predictions):\n",
    "    return accuracy_score(true_values, predictions)\n",
    "\n",
    "# Precisão\n",
    "def calcular_precisao(true_values, predictions, average='binary'):\n",
    "    return precision_score(true_values, predictions, average=average)\n",
    "\n",
    "# Recall\n",
    "def calcular_recall(true_values, predictions, average='binary'):\n",
    "    return recall_score(true_values, predictions, average=average)\n",
    "\n",
    "# F1-Score\n",
    "def calcular_f1(true_values, predictions, average='binary'):\n",
    "    return f1_score(true_values, predictions, average=average)\n",
    "\n",
    "# Top-K Accuracy\n",
    "def calcular_top_k_accuracy(true_values, predictions_probs, k=3):\n",
    "    return top_k_accuracy_score(true_values, predictions_probs, k=k)\n",
    "\n",
    "def calcular_mae(true_values, predictions):\n",
    "    return mean_absolute_error(true_values, predictions)\n",
    "\n",
    "def calcular_mse(true_values, predictions):\n",
    "    return mean_squared_error(true_values, predictions)\n",
    "\n",
    "def calcular_r2(true_values, predictions):\n",
    "    return r2_score(true_values, predictions)\n",
    "\n",
    "def calcular_kendalltau(true_values, predictions):\n",
    "    return kendalltau(true_values, predictions)\n",
    "\n",
    "def calcular_spearmanr(true_values, predictions):\n",
    "    return spearmanr(true_values, predictions)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para testar o modelo\n",
    "'''def testar_modelo(modelo, files_test, usar_labels_value):\n",
    "    res, clique, labels = ler_arquivos(files_test, usar_labels_value)\n",
    "    combined_input, labels = combinar_entrada(res, clique, labels)\n",
    "    x_test, y_test = estruturar_entrada(combined_input, labels)\n",
    "    # Prever usando o modelo\n",
    "    previsoes = modelo.predict(x_test)\n",
    "    # Calcular métricas\n",
    "    acuracia = calcular_acuracia(y_test, previsoes)\n",
    "    print(f\"Acurácia do modelo: {acuracia:.2f}\")\n",
    "    precisao = calcular_precisao(y_test, previsoes)\n",
    "    print(f\"Precisão do modelo: {precisao:.2f}\")\n",
    "    recall = calcular_recall(y_test, previsoes)\n",
    "    print(f\"Recall do modelo: {recall:.2f}\")\n",
    "    f1 = calcular_f1(y_test, previsoes)\n",
    "    print(f\"F1 do modelo: {f1:.2f}\")\n",
    "    top_k = calcular_top_k_accuracy(y_test, previsoes)\n",
    "    print(f\"Top K do modelo: {top_k:.2f}\")\n",
    "    mae = calcular_mae(y_test, previsoes)\n",
    "    print(f\"MAE do modelo: {mae:.2f}\")\n",
    "    mse = calcular_mse(y_test, previsoes)\n",
    "    print(f\"MSE do modelo: {mse:.2f}\")\n",
    "    r2 = calcular_r2(y_test, previsoes)\n",
    "    print(f\"R2 do modelo: {r2:.2f}\")\n",
    "    # Correlação de Kendall\n",
    "    tau, _ = calcular_kendalltau(y_test, previsoes)\n",
    "    print(f\"Kendall's Tau: {tau:.4f}\")\n",
    "    # Correlação de Spearman\n",
    "    rho, _ = calcular_spearmanr(y_test, previsoes)\n",
    "    print(f\"Spearman Correlation: {rho:.4f}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82794f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''print(\"Carregando o modelo branch...\")\n",
    "modelo_branch = load_model(final_model_dnn_path)\n",
    "print(\"Testando o modelo branch...\")\n",
    "testar_modelo(modelo_branch, files_teste, False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e25cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''print(\"Carregando o modelo bound...\")\n",
    "modelo_bound = load_model(final_model_value_path)\n",
    "print(\"Testando o modelo bound...\")\n",
    "testar_modelo(modelo_bound, files_teste, True)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
