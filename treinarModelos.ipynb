{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a8cd5c9-fee6-4564-8736-438c67979782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e151c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpa a sessão antes de construir o modelo\n",
    "K.clear_session()\n",
    "\n",
    "# Configurações de GPU\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "# Define a política de precisão mista\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1128523a-a737-4d05-8a04-4d461db04a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_pointer(fp, tam):\n",
    "    lines = [ll.strip() for ll in fp]\n",
    "    ii = 0\n",
    "    labels = []\n",
    "    res = []\n",
    "    cli = []\n",
    "    numLinhas = 0\n",
    "    while ii < len(lines):\n",
    "        line = lines[ii]\n",
    "        #contando o numero de vertices do grafo\n",
    "        if \"cliqueatual\" not in line:\n",
    "            ii += 1\n",
    "            numLinhas += 1\n",
    "            continue\n",
    "\n",
    "        #pegando a clique atual\n",
    "        if ii+1 >= len(lines):\n",
    "            break\n",
    "        line = line[3:]\n",
    "        spritado = line.split()\n",
    "        clique = [int(elem) for elem in spritado[1:]]\n",
    "        if(numLinhas < tam):\n",
    "            dif = tam - numLinhas\n",
    "            clique.extend([0]*dif)\n",
    "        cli.append(clique)\n",
    "\n",
    "        #criando o vetor de movimento\n",
    "        line = lines[ii+1]\n",
    "        sp = line.split()\n",
    "        mv = int(sp[-1])\n",
    "        label = [0] * tam\n",
    "        label[mv] = 1\n",
    "        labels.append(label)\n",
    "\n",
    "        #lendo o grafo\n",
    "        cells = []\n",
    "        for tt in range(numLinhas, 0, -1):\n",
    "            cell_line = lines[ii - tt][3:]\n",
    "            cells.extend([int(float(cc)) for cc in cell_line.split(\", \")])\n",
    "            if(numLinhas < tam):\n",
    "                dif = tam - numLinhas\n",
    "                cells.extend([0]*dif)\n",
    "        while len(cells) < tam * tam:\n",
    "            cells.extend([0]*tam)\n",
    "\n",
    "        #cells = np.reshape(cells,((tam,  -1)))\n",
    "        #cells = np.transpose(cells)\n",
    "        #cells = np.reshape(cells, -1)\n",
    "        res.append(cells)\n",
    "        ii += (numLinhas+2)\n",
    "    labels_v = list(range(len(labels),0, -1))\n",
    "    return (res, cli, labels, labels_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ec6a7c5-ea3e-47a2-a1d3-d168e03916d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dir(ddir, tam):\n",
    "    res = []\n",
    "    cli = []\n",
    "    labels = []\n",
    "    labels_v = []\n",
    "    random.seed(42)\n",
    "    files = sorted([os.path.basename(ii) for ii in glob.glob(\"{0}/*.dimacs\".format(ddir))])\n",
    "    random.shuffle(files)\n",
    "    random.seed()\n",
    "    i = 0\n",
    "    for ff in files:\n",
    "        with open(os.path.join(ddir,ff), 'r') as fp:\n",
    "            rr, cc, ll, ll_v = parse_file_pointer(fp, tam)\n",
    "            res.extend(rr)\n",
    "            cli.extend(cc)\n",
    "            labels.extend(ll)\n",
    "            labels_v.extend(ll_v)\n",
    "        i+=1\n",
    "        if i > 100:\n",
    "            break\n",
    "    return res, cli, labels, labels_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae619989-ab64-4168-9a12-bc575a916901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class printbatch(callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        logging.info(\"Epoch: \"+ str(epoch))\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logging.info(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81e2016e-cf70-4e08-9a80-9d0159c2ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggerWriter:\n",
    "    def __init__(self, level):\n",
    "        self.level = level\n",
    "\n",
    "    def write(self, message):\n",
    "        if message != '\\n':\n",
    "            self.level(message)\n",
    "\n",
    "    def flush(self):\n",
    "        self.level(sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2809c21c-f5cb-4135-9ae5-e19e265a00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(data, clique, labels, tam, output_path, shared_layer_multipliers, layer_multipliers, batch_size, learning_rate):\n",
    "    shared_layer_multipliers = [x for x in shared_layer_multipliers if x != 0]\n",
    "\n",
    "    # Definir camadas de entrada\n",
    "    inputArray = [Input(shape=(tam,)) for _ in range(tam + 1)]\n",
    "\n",
    "    # Camadas densas compartilhadas\n",
    "    layer = inputArray\n",
    "    for i in range(len(shared_layer_multipliers)):\n",
    "        shared_dense = Dense(tam * shared_layer_multipliers[i], activation='relu')\n",
    "        layer = [shared_dense(l) for l in layer]\n",
    "\n",
    "    # Concatenar os vetores processados\n",
    "    merged_vector = Concatenate(axis=-1)(layer)\n",
    "\n",
    "    #camadas internas\n",
    "    layer = merged_vector\n",
    "    for i in range(len(layer_multipliers)):\n",
    "        layer = Dense((tam+1)*layer_multipliers[i],activation='relu')(layer)\n",
    "\n",
    "    #camada de saida\n",
    "    output_layer = Dense(tam, activation='softmax')(layer)\n",
    "\n",
    "    #compilar modelo\n",
    "    model = Model(inputs=inputArray, outputs=output_layer)\n",
    "    adam = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Combine `data` e `clique` ao longo da segunda dimensão\n",
    "    combined_input = np.array([np.hsplit(np.concatenate([data[i], clique[i]]), tam + 1) for i in range(len(clique))])\n",
    "    # Dividir a entrada em uma lista de 151 tensores de forma (batch_size, tam)\n",
    "    combined_input_list = [combined_input[:, i, :] for i in range(combined_input.shape[1])]\n",
    "    # Converter elementos para tensores\n",
    "    combined_input_list = [tf.convert_to_tensor(arr) for arr in combined_input_list]\n",
    "    print(type(combined_input_list))\n",
    "    print(type(combined_input_list[0]))\n",
    "\n",
    "    #Verificando as formas\n",
    "    print(f\"combined_input shape: ({len(combined_input_list)}, {len(combined_input_list[0])}, {len(combined_input_list[0][0])})\")\n",
    "    print(f\"labels shape: {labels.shape}\")\n",
    "\n",
    "    #treinar modelo\n",
    "    now = datetime.now()\n",
    "    model.fit(combined_input_list, labels, epochs=5, batch_size=batch_size, validation_split=0.2, verbose=1,\n",
    "              callbacks=[printbatch(), EarlyStopping(monitor='val_loss', patience=50, verbose=0),\n",
    "                         ModelCheckpoint(os.path.join(output_path, \"models\", \"dnn_model_\" + str(tam) + \"_\" + str(now.day) + \".\" +\n",
    "                                         str(now.month) + \".\" + str(now.year) + \"_\" + \"_{epoch:02d}-{val_loss:.2f}.keras\"),\n",
    "                                         monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')])\n",
    "    model.save(os.path.join(output_path,\"dnn_model_\" + str(tam) + \"_\" + str(now.day) + \".\" +str(now.month) + \".\" + str(now.year) + \"_.keras\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b339dcc-d277-407b-8efc-e54f05d09306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_value(data, clique, labels, tam, output_path, shared_layer_multipliers, layer_multipliers, batch_size, learning_rate):\n",
    "    shared_layer_multipliers = [x for x in shared_layer_multipliers if x != 0]\n",
    "\n",
    "    # Definir camadas de entrada\n",
    "    inputArray = [Input(shape=(tam,)) for _ in range(tam + 1)]\n",
    "\n",
    "    # Camadas densas compartilhadas\n",
    "    layer = inputArray\n",
    "    for i in range(len(shared_layer_multipliers)):\n",
    "        shared_dense = Dense(tam * shared_layer_multipliers[i], activation='relu')\n",
    "        layer = [shared_dense(l) for l in layer]\n",
    "\n",
    "    # Concatenar os vetores processados\n",
    "    merged_vector = Concatenate(axis=-1)(layer)\n",
    "\n",
    "    #camadas internas\n",
    "    layer = merged_vector\n",
    "    for i in range(len(layer_multipliers)):\n",
    "        layer = Dense(tam*layer_multipliers[i],activation='relu')(layer)\n",
    "\n",
    "    #camada de saida\n",
    "    output_layer = Dense(1)(layer)\n",
    "\n",
    "    #compilar modelo\n",
    "    model = Model(inputs=inputArray, outputs=output_layer)\n",
    "    adam = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=adam, loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Combine `data` e `clique` ao longo da segunda dimensão\n",
    "    combined_input = np.array([np.hsplit(np.concatenate([data[i], clique[i]]), tam + 1) for i in range(len(clique))])\n",
    "    # Dividir a entrada em uma lista de 151 tensores de forma (batch_size, tam)\n",
    "    combined_input_list = [combined_input[:, i, :] for i in range(combined_input.shape[1])]\n",
    "    # Converter elementos para tensores\n",
    "    combined_input_list = [tf.convert_to_tensor(arr) for arr in combined_input_list]\n",
    "\n",
    "    #treinar modelo\n",
    "    now = datetime.now()\n",
    "    model.fit(combined_input_list, labels, epochs=5, batch_size=batch_size, validation_split=0.2, verbose=1,\n",
    "              callbacks=[printbatch(),\n",
    "                         EarlyStopping(monitor='val_loss', patience=50, verbose=0),\n",
    "                         ModelCheckpoint(os.path.join(output_path, \"models\", \"dnn_value_model_\" + str(tam) + \"x\" + \"_\" + str(now.day) + \".\" +\n",
    "                                        str(now.month) + \".\" + str(now.year) + \"_\" + \"_{epoch:02d}-{val_loss:.2f}.keras\"), monitor='val_loss',\n",
    "                                         verbose=0, save_best_only=True, save_weights_only=False, mode='auto')])\n",
    "    model.save(os.path.join(output_path,\"dnn_value_model_\" +str(tam) + \"_\" +str(now.day) + \".\" +str(now.month) +\".\"+str(now.year)+\"_.keras\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6f3a334-c5de-4170-b29e-9ca2a5a25ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leu tudo\n",
      "<class 'list'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "combined_input shape: (151, 1115, 150)\n",
      "labels shape: (1115, 150)\n",
      "Epoch 1/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 6s/step - accuracy: 0.0119 - loss: 5.1555 - val_accuracy: 0.0314 - val_loss: 5.2543\n",
      "Epoch 2/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 555ms/step - accuracy: 0.0163 - loss: 5.0832 - val_accuracy: 0.0269 - val_loss: 4.9681\n",
      "Epoch 3/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 572ms/step - accuracy: 0.0160 - loss: 4.9190 - val_accuracy: 0.0269 - val_loss: 4.9525\n",
      "Epoch 4/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 949ms/step - accuracy: 0.0191 - loss: 4.7862 - val_accuracy: 0.0045 - val_loss: 4.9357\n",
      "Epoch 5/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.0341 - loss: 4.6460 - val_accuracy: 0.0045 - val_loss: 4.9563\n",
      "rede 1 treinada\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igor/anaconda3/envs/tcc/lib/python3.9/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_760', 'keras_tensor_761', 'keras_tensor_762', 'keras_tensor_763', 'keras_tensor_764', 'keras_tensor_765', 'keras_tensor_766', 'keras_tensor_767', 'keras_tensor_768', 'keras_tensor_769', 'keras_tensor_770', 'keras_tensor_771', 'keras_tensor_772', 'keras_tensor_773', 'keras_tensor_774', 'keras_tensor_775', 'keras_tensor_776', 'keras_tensor_777', 'keras_tensor_778', 'keras_tensor_779', 'keras_tensor_780', 'keras_tensor_781', 'keras_tensor_782', 'keras_tensor_783', 'keras_tensor_784', 'keras_tensor_785', 'keras_tensor_786', 'keras_tensor_787', 'keras_tensor_788', 'keras_tensor_789', 'keras_tensor_790', 'keras_tensor_791', 'keras_tensor_792', 'keras_tensor_793', 'keras_tensor_794', 'keras_tensor_795', 'keras_tensor_796', 'keras_tensor_797', 'keras_tensor_798', 'keras_tensor_799', 'keras_tensor_800', 'keras_tensor_801', 'keras_tensor_802', 'keras_tensor_803', 'keras_tensor_804', 'keras_tensor_805', 'keras_tensor_806', 'keras_tensor_807', 'keras_tensor_808', 'keras_tensor_809', 'keras_tensor_810', 'keras_tensor_811', 'keras_tensor_812', 'keras_tensor_813', 'keras_tensor_814', 'keras_tensor_815', 'keras_tensor_816', 'keras_tensor_817', 'keras_tensor_818', 'keras_tensor_819', 'keras_tensor_820', 'keras_tensor_821', 'keras_tensor_822', 'keras_tensor_823', 'keras_tensor_824', 'keras_tensor_825', 'keras_tensor_826', 'keras_tensor_827', 'keras_tensor_828', 'keras_tensor_829', 'keras_tensor_830', 'keras_tensor_831', 'keras_tensor_832', 'keras_tensor_833', 'keras_tensor_834', 'keras_tensor_835', 'keras_tensor_836', 'keras_tensor_837', 'keras_tensor_838', 'keras_tensor_839', 'keras_tensor_840', 'keras_tensor_841', 'keras_tensor_842', 'keras_tensor_843', 'keras_tensor_844', 'keras_tensor_845', 'keras_tensor_846', 'keras_tensor_847', 'keras_tensor_848', 'keras_tensor_849', 'keras_tensor_850', 'keras_tensor_851', 'keras_tensor_852', 'keras_tensor_853', 'keras_tensor_854', 'keras_tensor_855', 'keras_tensor_856', 'keras_tensor_857', 'keras_tensor_858', 'keras_tensor_859', 'keras_tensor_860', 'keras_tensor_861', 'keras_tensor_862', 'keras_tensor_863', 'keras_tensor_864', 'keras_tensor_865', 'keras_tensor_866', 'keras_tensor_867', 'keras_tensor_868', 'keras_tensor_869', 'keras_tensor_870', 'keras_tensor_871', 'keras_tensor_872', 'keras_tensor_873', 'keras_tensor_874', 'keras_tensor_875', 'keras_tensor_876', 'keras_tensor_877', 'keras_tensor_878', 'keras_tensor_879', 'keras_tensor_880', 'keras_tensor_881', 'keras_tensor_882', 'keras_tensor_883', 'keras_tensor_884', 'keras_tensor_885', 'keras_tensor_886', 'keras_tensor_887', 'keras_tensor_888', 'keras_tensor_889', 'keras_tensor_890', 'keras_tensor_891', 'keras_tensor_892', 'keras_tensor_893', 'keras_tensor_894', 'keras_tensor_895', 'keras_tensor_896', 'keras_tensor_897', 'keras_tensor_898', 'keras_tensor_899', 'keras_tensor_900', 'keras_tensor_901', 'keras_tensor_902', 'keras_tensor_903', 'keras_tensor_904', 'keras_tensor_905', 'keras_tensor_906', 'keras_tensor_907', 'keras_tensor_908', 'keras_tensor_909', 'keras_tensor_910']. Received: the structure of inputs=('*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*')\n",
      "  warnings.warn(\n",
      "2024-11-30 14:15:33.196511: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'loop_pad_fusion_423', 700 bytes spill stores, 700 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_pad_fusion_422', 288 bytes spill stores, 288 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_maximum_fusion_10', 304 bytes spill stores, 304 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_maximum_fusion_8', 276 bytes spill stores, 276 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 158.1673 - mae: 7.3520"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 14:16:12.017325: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'loop_pad_fusion_423', 748 bytes spill stores, 748 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_pad_fusion_422', 284 bytes spill stores, 284 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_maximum_fusion_10', 268 bytes spill stores, 268 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_maximum_fusion_8', 276 bytes spill stores, 276 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - loss: 159.4083 - mae: 7.3393  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 14:16:21.537128: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'loop_maximum_fusion_11', 292 bytes spill stores, 292 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'loop_maximum_fusion_9', 276 bytes spill stores, 276 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 7s/step - loss: 160.4010 - mae: 7.3291 - val_loss: 67.3766 - val_mae: 6.8924\n",
      "Epoch 2/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 190ms/step - loss: 36.0221 - mae: 4.7268 - val_loss: 22.3800 - val_mae: 3.7609\n",
      "Epoch 3/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 208ms/step - loss: 14.0268 - mae: 2.9952 - val_loss: 17.9312 - val_mae: 3.3170\n",
      "Epoch 4/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 235ms/step - loss: 11.6649 - mae: 2.6953 - val_loss: 16.5175 - val_mae: 3.3223\n",
      "Epoch 5/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 241ms/step - loss: 10.6425 - mae: 2.6398 - val_loss: 16.2352 - val_mae: 3.2332\n",
      "rede 2 treinada\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    output_path = \"modelos\" #caminha onde é salvo o modelo\n",
    "    labeled_data_dir = \"train_graphs\" #caminho dos dados para treinar o modelo\n",
    "    param_v_a_1 = [4, 3, 2] #camadas compartilhadas rede bound\n",
    "    param_v_a_2 = [3, 2, 2] #camadas ocultas\n",
    "    param_p_a_1 = [6, 4, 3] #camadas compartilhadas rede brach\n",
    "    param_p_a_2 = [9, 6, 2] #camadas ocultas\n",
    "    param_p_b = 100 #batch size\n",
    "    param_v_b =  100\n",
    "    param_p_l = 0.001 #taxa de aprendizado\n",
    "    param_v_l = 0.001\n",
    "    tam = 150\n",
    "    use_value_model = True #se vai treinar a rede de bound\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    #treinar rede branch\n",
    "    res, clique, labels, labels_v = parse_dir(labeled_data_dir, tam)\n",
    "    print(\"leu tudo\")\n",
    "    learn(np.array(res), np.array(clique), np.array(labels), tam, output_path, param_p_a_1, param_p_a_2, param_p_b, param_p_l)\n",
    "    print(\"rede 1 treinada\")\n",
    "    #treinar rede bound\n",
    "    if use_value_model:\n",
    "        learn_value(np.array(res), np.array(clique), np.array(labels_v), tam, output_path, param_v_a_1, param_v_a_2, param_v_b, param_v_l)\n",
    "        print(\"rede 2 treinada\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
